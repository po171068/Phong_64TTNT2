{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nD73BJS7ueZ",
        "outputId": "4a03e1a9-5bd1-4105-c351-a674bf934c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.3435\n",
            "Epoch 100, Loss: 0.0137\n",
            "Epoch 200, Loss: 0.0052\n",
            "Epoch 300, Loss: 0.0029\n",
            "Epoch 400, Loss: 0.0021\n",
            "Dịch: bạn khỏe không → how are you\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "data = [\n",
        "    (\"xin chào\", \"hello\"),\n",
        "    (\"cảm ơn\", \"thank you\"),\n",
        "    (\"tạm biệt\", \"goodbye\"),\n",
        "    (\"tôi yêu bạn\", \"i love you\"),\n",
        "    (\"bạn khỏe không\", \"how are you\"),\n",
        "]\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
        "    idx = 3\n",
        "    for s in sentences:\n",
        "        for w in s.split():\n",
        "            if w not in vocab:\n",
        "                vocab[w] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([s for s, _ in data])\n",
        "tgt_vocab = build_vocab([t for _, t in data])\n",
        "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab[\"<sos>\"]] + [vocab[w] for w in sentence.split()] + [vocab[\"<eos>\"]]\n",
        "\n",
        "def pad(seq, max_len):\n",
        "    return seq + [0] * (max_len - len(seq))\n",
        "\n",
        "src_data = [encode(s, src_vocab) for s, _ in data]\n",
        "tgt_data = [encode(t, tgt_vocab) for _, t in data]\n",
        "\n",
        "src_max_len = max(len(s) for s in src_data)\n",
        "tgt_max_len = max(len(t) for t in tgt_data)\n",
        "\n",
        "src_tensor = torch.tensor([pad(s, src_max_len) for s in src_data])\n",
        "tgt_tensor = torch.tensor([pad(t, tgt_max_len) for t in tgt_data])\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(1, 100, d_model))  # vị trí (simple)\n",
        "        self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.src_embed(src) + self.pos_enc[:, :src.size(1)]\n",
        "        tgt_emb = self.tgt_embed(tgt) + self.pos_enc[:, :tgt.size(1)]\n",
        "        out = self.transformer(src_emb.permute(1,0,2), tgt_emb.permute(1,0,2))\n",
        "        out = self.fc_out(out.permute(1,0,2))\n",
        "        return out\n",
        "model = TransformerModel(len(src_vocab), len(tgt_vocab))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src_tensor, tgt_tensor[:, :-1])\n",
        "    loss = criterion(output.reshape(-1, len(tgt_vocab)), tgt_tensor[:, 1:].reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "test = \"bạn khỏe không\"\n",
        "test_enc = torch.tensor([pad(encode(test, src_vocab), src_max_len)])\n",
        "tgt_start = torch.tensor([[tgt_vocab[\"<sos>\"]]])\n",
        "\n",
        "for i in range(5):\n",
        "    out = model(test_enc, tgt_start)\n",
        "    next_token = out.argmax(-1)[:, -1]\n",
        "    tgt_start = torch.cat([tgt_start, next_token.unsqueeze(0)], dim=1)\n",
        "    if next_token.item() == tgt_vocab[\"<eos>\"]:\n",
        "        break\n",
        "\n",
        "translated = \" \".join(inv_tgt_vocab[idx.item()] for idx in tgt_start[0][1:-1])\n",
        "print(\"Dịch:\", test, \"→\", translated)\n"
      ]
    }
  ]
}